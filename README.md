# Turkish Poetry Generation using turkish-gpt2 Model

This project aims to generate Turkish poetry using the **"turkish-gpt2-large-750m-instruct-v0.1"** model developed by YÄ±ldÄ±z Technical University.  
Based on the GPT-2 architecture and powered by the Hugging Face Transformers library, this Turkish language model creates meaningful and creative poetic lines from a given prompt.

## Project Features

- **Model:** ytu-ce-cosmos/turkish-gpt2-large-750m-instruct-v0.1 (Turkish GPT-2 large language model)
- **Input:** A starting poetic line or phrase provided by the user
- **Output:** Fluent and meaningful poetic verses generated by the model
- **Technology:** Text tokenization and generation using Hugging Face Transformers and PyTorch  
- **Customization:** Sampling parameters such as `top_k`, `top_p`, `temperature`, and `repetition_penalty` allow control over creativity and repetition
- **Result:** Generated poems are printed to the console and saved with a timestamp in a `poem.txt` file

## Usage

The example prompt used in this project is: KenetlenmiÅŸsin kalbime, ilmek ilmek
The model processes this input and generates a poem of approximately 100 tokens.

## Purpose

The main goal is to promote and utilize local resources for creative text generation in the field of Turkish Natural Language Processing (NLP).  
This project demonstrates a practical application of Turkish poetry generation using large language models.

## Developer Team

The GPT-2-based Turkish language model used in this project was developed by the Cosmos research group at YÄ±ldÄ±z Technical University.  
For more information and related projects, please visit:

ðŸ”— [https://cosmos.yildiz.edu.tr/](https://cosmos.yildiz.edu.tr/)
